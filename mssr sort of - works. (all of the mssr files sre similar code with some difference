import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the Spatial Rearrangement Unit (SRU)
class SpatialRearrangementUnit(nn.Module):
    def __init__(self, window_size):
        super(SpatialRearrangementUnit, self).__init__()
        self.window_size = window_size
        self.step = window_size // 2

    def rearrange_dimension(self, x, dim):
        chunk_size = self.window_size // 2
        chunks = list(x.split(chunk_size, dim=dim))
        num_chunks = (x.size(dim) - 2 * chunk_size) // chunk_size
        num_groups = num_chunks // 2
        new_chunks = []
        for i in range(1, num_groups + 1):
            first = chunks[2 * i - 2]
            second = chunks[2 * i + 1]
            new_chunks.append(torch.cat([first, second], dim=dim))
        return torch.cat(new_chunks, dim=dim)

    def forward(self, x):
        B, C, H, W = x.shape
        chunk_size = self.window_size // 2

        # Width-direction padding and rearrangement
        left_pad = x[:, :, :, :chunk_size]
        right_pad = x[:, :, :, -chunk_size:]
        x_padded_w = torch.cat([left_pad, x, right_pad], dim=3)
        x_width = self.rearrange_dimension(x_padded_w, dim=3)

        # Height-direction padding and rearrangement
        top_pad = x_width[:, :, :chunk_size, :]
        bottom_pad = x_width[:, :, -chunk_size:, :]
        x_padded_h = torch.cat([top_pad, x_width, bottom_pad], dim=2)
        x_final = self.rearrange_dimension(x_padded_h, dim=2)
        return x_final

# Define the Multiscale Spatial Rearrangement Network (MSSR)
class MSSRNet(nn.Module):
    def __init__(self, in_channels, hidden_dim, out_channels, patch_size):
        super(MSSRNet, self).__init__()
        self.patch_size = patch_size

        # Apply SRUs with different window sizes (2, 4, 8)
        self.sru_small = SpatialRearrangementUnit(window_size=2)
        self.sru_medium = SpatialRearrangementUnit(window_size=4)
        self.sru_large = SpatialRearrangementUnit(window_size=8)

        # MLP layer to combine features
        flattened_dim = in_channels * patch_size * patch_size
        self.mlp = nn.Sequential(
            nn.Linear(flattened_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, flattened_dim)
        )

    def forward(self, x):
        # Apply SRUs to the input image at multiple scales
        x_small = self.sru_small(x)
        x_medium = self.sru_medium(x)
        x_large = self.sru_large(x)
        
        # Combine the different scale features
        x_combined = x_small + x_medium + x_large
        
        # Flatten and apply MLP for restoration
        B, C, H, W = x_combined.shape
        flattened = x_combined.view(B, -1)
        
        restored = self.mlp(flattened)
        restored = restored.view(B, C, H, W)
        return restored

# Example of how to use this model with a dummy image
if __name__ == '__main__':
    torch.set_printoptions(linewidth=200)

    # Test with a dummy image (4x4)
    dummy_input = torch.arange(1, 17).reshape(1, 1, 4, 4).float()
    print("Test Case - 4x4 input")
    print("Input:")
    print(dummy_input)

    # Initialize the MSSR network
    model = MSSRNet(in_channels=1, hidden_dim=64, out_channels=1, patch_size=4)

    # Forward pass through the model
    output = model(dummy_input)
    print("\nOutput:")
    print(output)
